{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers pillow sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffe441f2c9449f28a974f22d7082c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062be9a580d247aa8d04d17ac3f2ac44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Use this function when downloading the first model\n",
    "'''\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "from PIL import Image\n",
    "\n",
    "def model_download():\n",
    "    # Need to modify path for model download\n",
    "    blip_model_dir = './model/Blip'\n",
    "    translate_model_dir = './model/M2M100'\n",
    "\n",
    "    # Load Blip model and processor\n",
    "    processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n",
    "    model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\n",
    "\n",
    "    processor.save_pretrained(blip_model_dir)\n",
    "    model.save_pretrained(blip_model_dir)\n",
    "    \n",
    "    # Load Translate model and processor (English to Korean)\n",
    "    translator_tokenizer = M2M100Tokenizer.from_pretrained('facebook/m2m100_418M')\n",
    "    translator_model = M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')\n",
    "\n",
    "    translator_tokenizer.save_pretrained(translate_model_dir)\n",
    "    translator_model.save_pretrained(translate_model_dir)\n",
    "\n",
    "\n",
    "model_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 영어 설명: a group of women in traditional dress dancing\n",
      "생성된 한국어 설명: ['전통적인 드레스 댄스에서 여성 그룹']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['전통적인 드레스 댄스에서 여성 그룹']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def predict(image_path):\n",
    "    # Saved model path\n",
    "    blip_model_dir = './model/Blip'\n",
    "    translate_model_dir = './model/M2M100'\n",
    "\n",
    "    # Load Blip model and processor\n",
    "    processor = BlipProcessor.from_pretrained(blip_model_dir)\n",
    "    model = BlipForConditionalGeneration.from_pretrained(blip_model_dir)\n",
    "\n",
    "    # Load Translate model and processor (English to Korean)\n",
    "    translator_tokenizer = M2M100Tokenizer.from_pretrained(translate_model_dir)\n",
    "    translator_model = M2M100ForConditionalGeneration.from_pretrained(translate_model_dir)\n",
    "\n",
    "    # Set Source Language and Target Language\n",
    "    translator_tokenizer.src_lang = \"en\"\n",
    "    translator_tokenizer.tgt_lang = \"ko\"\n",
    "\n",
    "    # Load Image and preprocessing\n",
    "    raw_image = Image.open(image_path).convert('RGB')\n",
    "    inputs = processor(raw_image, return_tensors='pt')\n",
    "\n",
    "    # Create English Caption\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length = 100, min_length = 5)\n",
    "\n",
    "    # Decode English Caption\n",
    "    english_caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"생성된 영어 설명:\", english_caption)\n",
    "\n",
    "    # Translate English to Korean\n",
    "    translator_inputs = translator_tokenizer(english_caption, return_tensors='pt')\n",
    "\n",
    "    translated_tokens = translator_model.generate(\n",
    "            **translator_inputs,\n",
    "            forced_bos_token_id=translator_tokenizer.get_lang_id(\"ko\")\n",
    "        )\n",
    "\n",
    "\n",
    "    # Decode Korean Caption\n",
    "    korean_caption = translator_tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
    "    print(\"생성된 한국어 설명:\", korean_caption)\n",
    "    \n",
    "    return korean_caption\n",
    "\n",
    "image_path = '178045.jpg'\n",
    "\n",
    "predict(image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
